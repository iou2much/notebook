{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.525 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "/usr/local/spark/python/pyspark/sql/context.py:535: UserWarning: load is deprecated. Use read.load() instead.\n",
      "  warnings.warn(\"load is deprecated. Use read.load() instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark 初始化完成\n"
     ]
    }
   ],
   "source": [
    "#引入spark\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "\n",
    "#sparkSQL\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "#结巴分词\n",
    "import jieba\n",
    "import jieba.analyse\n",
    "jieba.enable_parallel(4)\n",
    "jieba.load_userdict('/notebooks/ciku/words-360w.res.txt')\n",
    "jieba.analyse.set_stop_words('/notebooks/ciku/chinese_stopword.txt')\n",
    "\n",
    "#BeautifulSoup,剔除HTML标签\n",
    "from BeautifulSoup import BeautifulSoup\n",
    "\n",
    "#图表生成\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#词云\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "#清洗规则\n",
    "import re \n",
    "facePatt = re.compile(r'\\[.*?\\]') \n",
    "namePatt = re.compile(r'^@.*') \n",
    "punct = set(u'''|▲/#:!),.:;?]}¢'\"、。〉》」』】〕〗〞︰︱︳﹐､﹒\n",
    "﹔﹕﹖﹗﹚﹜﹞！），．：；？｜｝︴︶︸︺︼︾﹀﹂﹄﹏､～￠\n",
    "々‖•·ˇˉ―--′’”([{£¥'\"‵〈《「『【〔〖（［｛￡￥〝︵︷︹︻\n",
    "︽︿﹁﹃﹙﹛﹝（｛“‘-—_…''')\n",
    "# 对str/unicode\n",
    "filterpunt = lambda s: ''.join(filter(lambda x: x not in punct, s))\n",
    "#分词，清洗\n",
    "def sp(row):\n",
    "    soup = BeautifulSoup(row.weibo_content).findAll(text=True)\n",
    "    res = list()\n",
    "    for ct in soup:\n",
    "        ct = namePatt.sub('',facePatt.sub('', filterpunt(ct)))\n",
    "        if ct is not None and ct != '':\n",
    "            res += jieba.cut(ct)\n",
    "    return res\n",
    "\n",
    "\n",
    "#初始化sparkContext\n",
    "sc = pyspark.SparkContext()\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "#生成停止词RDD\n",
    "stopWordRDD = sc.textFile(\"file:///notebooks/ciku/chinese_stopword.txt\")\n",
    "stopWordRDD = sc.parallelize([(word,1) for word in stopWordRDD.collect()])\n",
    "\n",
    "#生成词典RDD\n",
    "dictSchema = StructType([ \\\n",
    "    StructField(\"word\",StringType(),True), \\\n",
    "    StructField(\"freq\",StringType(),True), \\\n",
    "    StructField(\"prop\",StringType(),True)\n",
    "])\n",
    "dictDF = sqlContext.load(source=\"com.databricks.spark.csv\", path = \"/notebooks/ciku/words-360w.res.txt\",schema = dictSchema,delimiter=\" \")\n",
    "dictRDD = dictDF.select('word','prop').rdd.map(tuple)\n",
    "\n",
    "if sqlContext:\n",
    "    print \"Spark 初始化完成\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df 创建成功\n"
     ]
    }
   ],
   "source": [
    "#定义数据结构\n",
    "customSchema = StructType([ \\\n",
    "    StructField(\"Id\",LongType(),True), \\\n",
    "    StructField(\"crawler_time\",TimestampType(),True), \\\n",
    "    StructField(\"crawler_time_stamp\",LongType(),True), \\\n",
    "    StructField(\"is_retweet\",ByteType(),True), \\\n",
    "    StructField(\"user_id\",LongType(),True), \\\n",
    "    StructField(\"nick_name\",StringType(),True), \\\n",
    "    StructField(\"tou_xiang\",StringType(),True), \\\n",
    "    StructField(\"user_type\",StringType(),True), \\\n",
    "    StructField(\"weibo_id\",LongType(),True), \\\n",
    "    StructField(\"weibo_content\",StringType(),True), \\\n",
    "    StructField(\"zhuan\",IntegerType(),True), \\\n",
    "    StructField(\"ping\",IntegerType(),True), \\\n",
    "    StructField(\"zhan\",IntegerType(),True), \\\n",
    "    StructField(\"url\",StringType(),True), \\\n",
    "    StructField(\"device\",StringType(),True), \\\n",
    "    StructField(\"locate\",StringType(),True), \\\n",
    "    StructField(\"time\",TimestampType(),True), \\\n",
    "    StructField(\"time_stamp\",LongType(),True), \\\n",
    "    StructField(\"r_user_id\",LongType(),True), \\\n",
    "    StructField(\"r_nick_name\",StringType(),True), \\\n",
    "    StructField(\"r_user_type\",StringType(),True), \\\n",
    "    StructField(\"r_weibo_id\",LongType(),True), \\\n",
    "    StructField(\"r_weibo_content\",StringType(),True), \\\n",
    "    StructField(\"r_zhuan\",IntegerType(),True), \\\n",
    "    StructField(\"r_ping\",IntegerType(),True), \\\n",
    "    StructField(\"r_zhan\",IntegerType(),True), \\\n",
    "    StructField(\"r_url\",StringType(),True), \\\n",
    "    StructField(\"r_device\",StringType(),True), \\\n",
    "    StructField(\"r_location\",StringType(),True), \\\n",
    "    StructField(\"r_time\",TimestampType(),True), \\\n",
    "    StructField(\"r_time_stamp\",LongType(),True), \\\n",
    "    StructField(\"pic_content\",StringType(),True)\n",
    "])\n",
    "\n",
    "#读入微博数据\n",
    "df = sqlContext.load(source=\"com.databricks.spark.csv\", path = \"/notebooks/weibo/weibo_freshdata.2016-05-01\",schema = customSchema,delimiter=\"\\t\")\n",
    "\n",
    "if df:\n",
    "    print \"df 创建成功\"\n",
    "#df.select(\"year\", \"model\").save(\"newcars.csv\", \"com.databricks.spark.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已选择 269094 条数据...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' has no length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-5aa92b722a15>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#数词\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mrdd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mprint\u001b[0m \u001b[1;34m'分成 %s 个词条...'\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mwcount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcountByKey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' has no length"
     ]
    }
   ],
   "source": [
    "#选择数据\n",
    "user = df[(df.time >= '2016-05-01 00:00:00') & (df.time <= '2016-05-01 03:00:00')]\n",
    "print '已选择 %s 条数据...'%(user.count())\n",
    "#分词\n",
    "test = user.select('weibo_content').map(sp).collect()\n",
    "#数词\n",
    "rdd = sc.parallelize([(word,1) for line in test for word in line])\n",
    "print '分成 %s 个词条...'%(rdd.count())\n",
    "\n",
    "wcount = sc.parallelize(rdd.filter(lambda x: len(x[0]) >1).countByKey().items())\n",
    "#排除停止词\n",
    "wcount = wcount.subtractByKey(stopWordRDD)#.collect()\n",
    "\n",
    "#排序\n",
    "top100RDD = wcount.sortBy(lambda x: x[1],ascending=False)\n",
    "top100 = top100RDD.collect()[:100]\n",
    "#top100[:10]\n",
    "\n",
    "#?????????\n",
    "\n",
    "#关联词性\n",
    "joinedRDD = top100RDD.leftOuterJoin(dictRDD)\n",
    "joinedRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top100RDD2 = joinedRDD.sortBy(lambda x: x[1][0],ascending=False)\n",
    "top1002 = top100RDD2.collect()[:100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top100N = top100RDD2.filter(lambda x : x[1][1] is not None and x[1][1].startswith('nr')).collect()[:100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.ndarray' does not have the buffer interface",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-c9d48092b941>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#wordcloud = WordCloud(font_path=\"/usr/local/lib/python2.7/dist-packages/wordcloud/yahei.ttf\").fit_words([(word,freq) for word,(freq,_) in top100N])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mwordcloud\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_font_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m40\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrelative_scaling\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfreq\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfreq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtop100N\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwordcloud\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"off\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/pypy2.7/dist-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mfit_words\u001b[1;34m(self, frequencies)\u001b[0m\n\u001b[0;32m    245\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m         \"\"\"\n\u001b[1;32m--> 247\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_from_frequencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfrequencies\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    248\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mgenerate_from_frequencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfrequencies\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/pypy2.7/dist-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_frequencies\u001b[1;34m(self, frequencies)\u001b[0m\n\u001b[0;32m    328\u001b[0m                 result = occupancy.sample_position(box_size[1] + self.margin,\n\u001b[0;32m    329\u001b[0m                                                    \u001b[0mbox_size\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmargin\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 330\u001b[1;33m                                                    random_state)\n\u001b[0m\u001b[0;32m    331\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfont_size\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/pypy2.7/dist-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36msample_position\u001b[1;34m(self, size_x, size_y, random_state)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msample_position\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mquery_integral_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintegral\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_array\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mwordcloud/query_integral_image.pyx\u001b[0m in \u001b[0;36mwordcloud.query_integral_image.query_integral_image (wordcloud/query_integral_image.c:1342)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/pypy2.7/dist-packages/wordcloud/query_integral_image.pypy-41-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mView.MemoryView.memoryview_cwrapper (wordcloud/query_integral_image.c:7042)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'numpy.ndarray' does not have the buffer interface"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8e7168>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.figure(figsize=(14,10))  \n",
    "\n",
    "#wordcloud = WordCloud(font_path=\"/usr/local/lib/python2.7/dist-packages/wordcloud/yahei.ttf\").fit_words([(word,freq) for word,(freq,_) in top100N])\n",
    "wordcloud = WordCloud(max_font_size=40, relative_scaling=.5).fit_words([(word,freq) for word,(freq,_) in top100N])\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "#导入数据\n",
    "data = [(Vectors.dense([0.0, 0.0]),), (Vectors.dense([1.0, 1.0]),),(Vectors.dense([9.0, 8.0]),), (Vectors.dense([8.0, 9.0]),)]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pypy",
   "language": "python",
   "name": "pypy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
